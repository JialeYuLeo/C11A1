{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Under this ensemble binary classification technique, error occurs when more than half of the result of the classifiers is wrong.\n",
    "So we should sums the possibilities of more than $\\frac{N-1}{2}$ classifiers produce wrong prediction.\n",
    "Hence we have:\n",
    "$$\n",
    "P_{\\text{ensemble}} = \\sum_{k= \\frac{N+1}{2}}^{N} {N \\choose k} \\epsilon^k (1-\\epsilon)^{N-k}\n",
    "$$\n",
    "\n",
    "Where $ {N \\choose k}$  representing the number of ways to choose k classifiers from N\\\n",
    "$\\epsilon^k$ representing the probability that k classifiers wrong\\\n",
    "$(1-\\epsilon)^{N-k}$  representing the probability that the remaining classifiers correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J1grKKjV-nzO",
    "outputId": "2b07549d-7ba6-4e10-dd9d-b232e58d6d35"
   },
   "outputs": [],
   "source": [
    "#pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0CQf65BS-nzT",
    "outputId": "37de0002-4906-4e99-a47f-609be2f2fade"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlease install this specific version of resampy for librosa to work without errors.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Please install this specific version of resampy for librosa to work without errors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7FE1qM7n-nzU",
    "outputId": "02c7b706-8c9c-42b4-ed25-b98f7cd915de"
   },
   "outputs": [],
   "source": [
    "#pip install resampy==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_hwFIUlFzTYw",
    "outputId": "9419a67b-5c1d-47e0-a7f5-ec9b9cad5bca"
   },
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GWrZdqrp2E98"
   },
   "outputs": [],
   "source": [
    "emotions ={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrgQMSEU-nzZ"
   },
   "source": [
    "### Data for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vSe0sBT12HgO"
   },
   "outputs": [],
   "source": [
    "def load_extract_features(data_path):\n",
    "\n",
    "    '''\n",
    "    load_extract_features() is a function that is used to load all the audio files one at a time, compute their features and return the features as well as the target values.\n",
    "\n",
    "    There are around 8-10 audio files which are corrupted. We hardcode zero values for such files in order to maintain consistency.\n",
    "\n",
    "    ['calm', 'happy'] emotion data is categorized into 'positive' and  ['angry', 'fearful'] into 'negative'\n",
    "\n",
    "    Returns:\n",
    "    1. Features\n",
    "    2. Binary Target Values\n",
    "    '''\n",
    "    final_features,target_emotions, binary_label = [],[], []\n",
    "    count = 0\n",
    "    \n",
    "    for i in glob.glob(data_path + \"/Actor_*/*.wav\"): #Loop to read every file.\n",
    "        \n",
    "        name = os.path.basename(i)\n",
    "        #We split the name of the file to understand the emotion associated with the file.\n",
    "        split = name.split(\"-\")\n",
    "        #We know that the third identifier is associated with the emotion of the audio file. Hence, we use [2] as it represents the third identifier.\n",
    "        emotion = emotions[split[2]]\n",
    "\n",
    "        #Below is the code to categorize the emotions into two classes to make this a binary problem.\n",
    "        if emotion in ['calm', 'happy']:\n",
    "            binary_label.append(0)\n",
    "        elif emotion in ['angry', 'fearful']:\n",
    "            binary_label.append(1)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        with soundfile.SoundFile(i) as audio:\n",
    "            waveform = audio.read(dtype=\"float32\")\n",
    "            sr = audio.samplerate\n",
    "            \n",
    "            #Below is the code to extract the Mel spectrogram features\n",
    "            #128 is the standard for machine learning applications using Mel spectrograms\n",
    "            m_feature = librosa.feature.melspectrogram(y=waveform, sr=sr, n_mels=128, fmax=sr / 2.0).T\n",
    "            melspectrogram = np.mean(m_feature,axis=0)\n",
    "            if melspectrogram.shape != (128,):\n",
    "                melspectrogram = np.zeros(128)\n",
    "            \n",
    "            #Below is the code to extract the chromagram features\n",
    "            stft_wave = librosa.stft(waveform)\n",
    "            stft = np.abs(stft_wave)\n",
    "            c_feature = librosa.feature.chroma_stft(S=stft, sr=sr).T\n",
    "            chromagram = np.mean(c_feature,axis=0)\n",
    "            \n",
    "            #12 is the number of pitch classes\n",
    "            if chromagram.shape != (12,):\n",
    "                chromagram = np.zeros(12)\n",
    "                \n",
    "            features=np.array([])\n",
    "            features=np.hstack((chromagram, melspectrogram))\n",
    "        \n",
    "            final_features.append(features)\n",
    "            target_emotions.append(emotion)\n",
    "            \n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                print(\"Processed Audio File Number: \", count)\n",
    "    \n",
    "    #We return the features and the binary target values.\n",
    "    return np.array(final_features), np.array(binary_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aER6S-_k2a9H",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Audio File Number:  100\n",
      "Processed Audio File Number:  200\n",
      "Processed Audio File Number:  300\n",
      "Processed Audio File Number:  400\n",
      "Processed Audio File Number:  500\n",
      "Processed Audio File Number:  600\n",
      "Processed Audio File Number:  700\n"
     ]
    }
   ],
   "source": [
    "#Please change the path below to the path of the folder saved on your computer.\n",
    "data_path = './Audio_Speech_Actors_01-24'\n",
    "X, binary_label = load_extract_features(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 140)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, binary_label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(514, 140)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def knn_classifier(X_train, y_train, X_test, k):\n",
    "    predictions = []\n",
    "\n",
    "    for test_point in X_test:\n",
    "        # Compute distances between the test point and all training points\n",
    "        distances = [euclidean_distance(test_point, train_point) for train_point in X_train]\n",
    "\n",
    "        # Sort by distance and return the indices of k closest neighbors\n",
    "        k_indices = np.argsort(distances)[:k]\n",
    "\n",
    "        # Extract the labels of the k nearest neighbors\n",
    "        k_nearest_labels = [y_train[i] for i in k_indices]\n",
    "\n",
    "        # Majority vote: most common class label among the k-nearest neighbors\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        predictions.append(most_common[0][0])\n",
    "\n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    total = len(y_true)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validation(X, y, k, num_folds):\n",
    "    kf = KFold(n_splits=num_folds,random_state=42,shuffle=True)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Assuming knn_classifier is your kNN classifier function\n",
    "        y_pred = knn_classifier(X_train, y_train, X_test, k)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Average accuracy across all folds\n",
    "    average_accuracy = sum(accuracies) / len(accuracies)\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=[]\n",
    "for i in range(3,30):\n",
    "    k=i\n",
    "    average_accuracy = cross_validation(X_train, y_train, k, num_folds=6)\n",
    "    score.append(average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = list(range(3, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = score.index(max(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7141130870953032"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_values[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using K=4 for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7283464566929134\n",
      "Elapsed time: 1.9667937755584717 seconds\n"
     ]
    }
   ],
   "source": [
    "# kNN prediction\n",
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "k = 4\n",
    "y_pred = knn_classifier(X_train, y_train, X_test, k)\n",
    "accuracy = calculate_accuracy(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part b PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_with_explained_variance(X, num_components):\n",
    "    # Standardize the data\n",
    "    mean_train = np.mean(X_train, axis=0)\n",
    "    X_standardized = (X - mean_train) / np.std(X, axis=0)\n",
    "\n",
    "    # Calculate the Covariance Matrix\n",
    "    covariance_matrix = np.cov(X_standardized, rowvar=False)\n",
    "\n",
    "    # Compute Eigenvalues and Eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "    # Sort Eigenvalues and Eigenvectors\n",
    "    sorted_index = np.argsort(eigenvalues)[::-1]\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_index]\n",
    "    sorted_eigenvalues = eigenvalues[sorted_index]\n",
    "\n",
    "    # Select a Subset from the Rearranged Eigenvalue Matrix\n",
    "    eigenvector_subset = sorted_eigenvectors[:, 0:num_components]\n",
    "\n",
    "    # Calculate Explained Variance\n",
    "    total_variance = sum(sorted_eigenvalues)\n",
    "    variance_explained = sum(sorted_eigenvalues[0:num_components]) / total_variance\n",
    "\n",
    "    return eigenvector_subset, mean_train, variance_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(X, components, mean):\n",
    "    # Standardize the data using the mean of the training data\n",
    "    X_std = (X - mean) / np.std(X, axis=0)\n",
    "\n",
    "    # Project the data onto the principal components\n",
    "    X_pca = np.dot(X_std, components)\n",
    "\n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: 0.9693805835676846\n"
     ]
    }
   ],
   "source": [
    "# Compute PCA on the training data\n",
    "components, mean_train, explained_variance = pca_with_explained_variance(X_train, num_components=40)\n",
    "print(\"Explained Variance:\", explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform both training and test sets\n",
    "X_train_pca = pca_transform(X_train, components, mean_train)\n",
    "X_test_pca = pca_transform(X_test, components, mean_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a low-dimensional data to cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7101231190150479"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score=[]\n",
    "for i in range(3,30):\n",
    "    k=i\n",
    "    average_accuracy = cross_validation(X_train_pca, y_train, k, num_folds=6)\n",
    "    score.append(average_accuracy)\n",
    "k_values = list(range(3, 30))\n",
    "max_index = score.index(max(score))\n",
    "max(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_values[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6811023622047244\n",
      "Elapsed time: 1.7357261180877686 seconds\n"
     ]
    }
   ],
   "source": [
    "# kNN prediction\n",
    "import time\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "k = 6\n",
    "y_pred = knn_classifier(X_train_pca, y_train, X_test_pca, k)\n",
    "accuracy = calculate_accuracy(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
